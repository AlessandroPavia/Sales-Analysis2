{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f397b84",
   "metadata": {},
   "source": [
    "IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b43e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import Levenshtein as lev\n",
    "from rapidfuzz import fuzz\n",
    "import re\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7abeac0",
   "metadata": {},
   "source": [
    "FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d114ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pulisci_stringhe_fuzz(df_temp, colonna, soglia_fuzz):\n",
    "    df_temp2=df_temp.copy() #poi usare df_temp2\n",
    "    colonna_mod = f'{colonna}_mod'\n",
    "    df_temp2[colonna_mod] = df_temp2[colonna].str.upper()\n",
    "\n",
    "\n",
    "    prima= set(df_temp2[colonna_mod].unique())\n",
    "\n",
    "    diz = df_temp2[colonna_mod].value_counts().to_dict()\n",
    "\n",
    "\n",
    "    for val1 in diz:\n",
    "        for val2 in diz:\n",
    "            if val1 != val2:\n",
    "                similarità = fuzz.ratio(val1, val2)\n",
    "                if similarità >= soglia_fuzz and diz[val1] >= diz[val2]:\n",
    "                    df_temp2.loc[df_temp2[colonna_mod] == val2, colonna_mod] = val1  #Sostituisci tutti i valori uguali a val2 nella colonna colonna_mod del DataFrame df_temp2 con val1.# creare nuova colonna invece di replace\n",
    "                elif similarità >=soglia_fuzz and diz[val1]<diz[val2]:\n",
    "                    df_temp2.loc[df_temp2[colonna_mod] == val1, colonna_mod] = val2\n",
    "\n",
    "   \n",
    "    dopo= set(df_temp2[colonna_mod].unique())\n",
    "    mancanti = prima - dopo\n",
    "    print(\"Valori rimossi o modificati:\", mancanti, len(mancanti))\n",
    "\n",
    "    varianti_dict = {\n",
    "    key: sorted(set(s.upper() for s in group[colonna].unique() if isinstance(s, str)))\n",
    "    for key, group in df_temp2.groupby(colonna_mod)}\n",
    "\n",
    "     \n",
    "\n",
    "    return df_temp2, varianti_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb44fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pulisci_stringhe_fuzz2(df_temp, colonna, soglia_fuzz,colonna_da_uguagliare):\n",
    "    df_temp2=df_temp.copy() #poi usare df_temp2\n",
    "    colonna_mod = f'{colonna}_mod'\n",
    "    df_temp2[colonna_mod] = df_temp2[colonna].str.upper()\n",
    "\n",
    "\n",
    "    prima= set(df_temp2[colonna_mod].unique())\n",
    "    diz = df_temp2[colonna_mod].value_counts().to_dict()\n",
    "\n",
    "\n",
    "    for val1 in diz:\n",
    "        for val2 in diz:\n",
    "            if val1 != val2:\n",
    "                similarità = fuzz.ratio(val1, val2)\n",
    "                if similarità >= soglia_fuzz and diz[val1] >= diz[val2]:\n",
    "                    stato_val1 = df_temp2.loc[df_temp2[colonna_mod] == val1, colonna_da_uguagliare].iloc[0]\n",
    "                    stato_val2 = df_temp2.loc[df_temp2[colonna_mod] == val2, colonna_da_uguagliare].iloc[0]\n",
    "                    if stato_val1 == stato_val2:\n",
    "                        df_temp2.loc[df_temp2[colonna_mod] == val2, colonna_mod] = val1  #Sostituisci tutti i valori uguali a val2 nella colonna colonna_mod del DataFrame df_temp2 con val1.# creare nuova colonna invece di replace\n",
    "                elif similarità >=soglia_fuzz and diz[val1]<diz[val2]:\n",
    "                    stato_val1 = df_temp2.loc[df_temp2[colonna_mod] == val1, colonna_da_uguagliare].iloc[0]\n",
    "                    stato_val2 = df_temp2.loc[df_temp2[colonna_mod] == val2, colonna_da_uguagliare].iloc[0]\n",
    "                    if stato_val1 == stato_val2:\n",
    "                        df_temp2.loc[df_temp2[colonna_mod] == val1, colonna_mod] = val2\n",
    "\n",
    "   \n",
    "    dopo= set(df_temp2[colonna_mod].unique())\n",
    "    mancanti = prima - dopo\n",
    "    print(\"Valori rimossi o modificati:\", mancanti, len(mancanti))\n",
    "\n",
    "    varianti_dict = {\n",
    "    key: sorted(set(s.upper() for s in group[colonna].unique() if isinstance(s, str)))\n",
    "    for key, group in df_temp2.groupby(colonna_mod)}\n",
    "     \n",
    "\n",
    "    return df_temp2, varianti_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade43419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pulisci_stringhe_lev(df_temp,colonna,soglia_lev):\n",
    "    df_temp2=df_temp.copy()\n",
    "    colonna_mod = f'{colonna}_mod'\n",
    "    df_temp2[colonna_mod] = df_temp2[colonna].str.upper()\n",
    "    \n",
    "\n",
    "    prima= set(df_temp2[colonna_mod].unique())\n",
    "    diz= df_temp2[colonna_mod].value_counts().to_dict()\n",
    "\n",
    "\n",
    "    for val1 in diz:\n",
    "        for val2 in diz:\n",
    "            if val1!=val2:\n",
    "                distanza=lev.distance(val1,val2)\n",
    "                if distanza <= soglia_lev and diz[val1]>=diz[val2]:\n",
    "                    df_temp2.loc[df_temp2[colonna_mod] == val2, colonna_mod] = val1\n",
    "                elif distanza <= soglia_lev and diz[val1]<diz[val2]:\n",
    "                    df_temp2.loc[df_temp2[colonna_mod] == val1, colonna_mod] = val2\n",
    "\n",
    "    \n",
    "    \n",
    "    dopo= set(df_temp2[colonna_mod].unique())\n",
    "    mancanti = prima - dopo\n",
    "    print(\"Valori rimossi o modificati:\", mancanti, len(mancanti))\n",
    "\n",
    "    dict = {\n",
    "    key: sorted(set(s.upper() for s in group[colonna].unique() if isinstance(s, str)))\n",
    "    for key, group in df_temp2.groupby(colonna_mod)}\n",
    "\n",
    "\n",
    "    return df_temp2, dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce1800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pulisci_stringhe_lev2(df_temp,colonna,soglia_lev,colonna_da_uguagliare):\n",
    "    df_temp2=df_temp.copy()\n",
    "    colonna_mod = f'{colonna}_mod'\n",
    "    df_temp2[colonna_mod] = df_temp2[colonna].str.upper()\n",
    "    \n",
    "\n",
    "    prima= set(df_temp2[colonna_mod].unique())\n",
    "    diz= df_temp2[colonna_mod].value_counts().to_dict()\n",
    "\n",
    "\n",
    "    for val1 in diz:\n",
    "        for val2 in diz:\n",
    "            if val1!=val2:\n",
    "                distanza=lev.distance(val1,val2)\n",
    "                if distanza <= soglia_lev and diz[val1]>=diz[val2]:\n",
    "                    stato_val1 = df_temp2.loc[df_temp2[colonna_mod] == val1, colonna_da_uguagliare].iloc[0]\n",
    "                    stato_val2 = df_temp2.loc[df_temp2[colonna_mod] == val2, colonna_da_uguagliare].iloc[0]\n",
    "                    if stato_val1 == stato_val2:\n",
    "                        df_temp2.loc[df_temp2[colonna_mod] == val2, colonna_mod] = val1 \n",
    "                elif distanza <= soglia_lev and diz[val1]<diz[val2]:\n",
    "                    stato_val1 = df_temp2.loc[df_temp2[colonna_mod] == val1, colonna_da_uguagliare].iloc[0]\n",
    "                    stato_val2 = df_temp2.loc[df_temp2[colonna_mod] == val2, colonna_da_uguagliare].iloc[0]\n",
    "                    if stato_val1 == stato_val2:\n",
    "                        df_temp2.loc[df_temp2[colonna_mod] == val1, colonna_mod] = val2\n",
    "\n",
    "    \n",
    "    \n",
    "    dopo= set(df_temp2[colonna_mod].unique())\n",
    "    mancanti = prima - dopo\n",
    "    print(\"Valori rimossi o modificati:\", mancanti, len(mancanti))\n",
    "\n",
    "    dict = {\n",
    "    key: sorted(set(s.upper() for s in group[colonna].unique() if isinstance(s, str)))\n",
    "    for key, group in df_temp2.groupby(colonna_mod)}\n",
    "\n",
    "    return df_temp2, dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5901e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pulisci_trattino(val):\n",
    "    val = str(val)\n",
    "    # Se c'è un \"-\" in mezzo, lo sostituisce con un \".\"\n",
    "    val = re.sub(r'(?<!^)-', '.', val) #(?<!^) è una negative lookbehind assertion: controlla che prima del trattino non ci sia l'inizio della stringa (^)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creo_dict(df,colonna_originale,colonna_modificata):\n",
    "    dizionario={key: list(group[colonna_originale].unique())\n",
    "    for key, group in df.groupby(colonna_modificata)}\n",
    "    return dizionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95339bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creo_dict2(df, colonna_originale, colonna_modificata):\n",
    "    dizionario = {\n",
    "        round(key, 2): list(set(\n",
    "            v if isinstance(v, (int, float)) else v\n",
    "            for v in group[colonna_originale]\n",
    "        ))\n",
    "        for key, group in df.groupby(colonna_modificata)\n",
    "    }\n",
    "    return dizionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22436a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniforma_date_order_date(df, colonna_originale1, colonna_originale2):\n",
    "    #creo una colonna in formato datetime e una in formato stringa\n",
    "    #quella in formato datetime mi serve per pulire il grosso degli errori\n",
    "    #quella in formato stringa mi serve per pulire gli errori come per esempio se il mese supera il 12, o se è 0 ecc\n",
    "    #quindi quando la colonna dt mi da errore, lavoro sulla colonna str\n",
    "    # scelgo come formato unico il formato YYYY/MM/DD\n",
    "    colonna1_dt=colonna_originale1 + '_dt'\n",
    "    colonna2_dt=colonna_originale2 + '_dt'\n",
    "    colonna1_def=colonna_originale1 + '_def'\n",
    "\n",
    "\n",
    "    df[colonna1_dt] = pd.to_datetime(df[colonna_originale1], errors='coerce')\n",
    "    df[colonna2_dt] = pd.to_datetime(df[colonna_originale2], errors='coerce')\n",
    "    \n",
    "\n",
    "    #Inizializzo colonna finale\n",
    "    df[colonna1_def] = df[colonna1_dt]  # dove non è NaT, la data è già buona\n",
    "\n",
    "    \n",
    "    for i, row in df[pd.isna(df[colonna1_dt])].iterrows():\n",
    "        raw_date = str(row[colonna_originale1]) #rendo la data in stringa così posso lavorarci\n",
    "        try:\n",
    "            parts = re.split(r'\\D+', raw_date.strip()) #cosi faccio lo split non solo sullo / ma qualsiasi simbolo non numerico\n",
    "            combinazioni = ['/'.join(p) for p in permutations(parts)] #trovo tutte le combinazioni possibili\n",
    "            date_possibili = pd.to_datetime(combinazioni, errors='coerce') #rendo ogni combinazione una data in formato dt\n",
    "            date_valide = date_possibili[date_possibili.notna() & (date_possibili <= row[colonna2_dt])] #controllo che la data possibile sia precedente alla data di spedizione\n",
    "            if not date_valide.empty:\n",
    "                distanze = row[colonna2_dt] - date_valide  # sarà sempre >= 0\n",
    "                indice_minimo = distanze.argmin()\n",
    "                data_probabile = date_valide.iloc[indice_minimo] #indice della differenza minima\n",
    "                df.at[i, colonna1_def] = data_probabile\n",
    "        except:\n",
    "            print(f'La riga {i} da errore')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7fdb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniforma_date_ship_date(df, colonna_originale1, colonna_originale2):\n",
    "    #creo una colonna in formato datetime e una in formato stringa\n",
    "    #quella in formato datetime mi serve per pulire il grosso degli errori\n",
    "    #quella in formato stringa mi serve per pulire gli errori come per esempio se il mese supera il 12, o se è 0 ecc\n",
    "    #quindi quando la colonna dt mi da errore, lavoro sulla colonna str\n",
    "    # scelgo come formato unico il formato YYYY/MM/DD\n",
    "    colonna1_dt=colonna_originale1 + '_dt'\n",
    "    colonna2_dt=colonna_originale2 + '_dt'\n",
    "    colonna1_def=colonna_originale1 + '_def'\n",
    "\n",
    "\n",
    "    df[colonna1_dt] = pd.to_datetime(df[colonna_originale1], errors='coerce')\n",
    "    df[colonna2_dt] = pd.to_datetime(df[colonna_originale2], errors='coerce')\n",
    "    \n",
    "\n",
    "    #Inizializzo colonna finale\n",
    "    df[colonna1_def] = df[colonna1_dt]  # dove non è NaT, la data è già buona\n",
    "\n",
    "    \n",
    "    for i, row in df[pd.isna(df[colonna1_dt])].iterrows():\n",
    "        raw_date = str(row[colonna_originale1]) #rendo la data in stringa così posso lavorarci\n",
    "        try:\n",
    "            parts = re.split(r'\\D+', raw_date.strip()) #cosi faccio lo split non solo sullo / ma qualsiasi simbolo non numerico\n",
    "            combinazioni = ['/'.join(p) for p in permutations(parts)] #trovo tutte le combinazioni possibili\n",
    "            date_possibili = pd.to_datetime(combinazioni, errors='coerce') #rendo ogni combinazione una data in formato dt\n",
    "            date_valide = date_possibili[date_possibili.notna() & (date_possibili >= row[colonna2_dt])] #controllo che la data possibile sia successiva alla data di ordine\n",
    "            if not date_valide.empty:\n",
    "                distanze = date_valide - row[colonna2_dt]  # sarà sempre >= 0\n",
    "                indice_minimo = distanze.argmin()\n",
    "                data_probabile = date_valide.iloc[indice_minimo] #indice della differenza minima\n",
    "                df.at[i, colonna1_def] = data_probabile\n",
    "        except:\n",
    "            print(f'La riga {i} da errore')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3c2bf6",
   "metadata": {},
   "source": [
    "GET DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leggi tutti i fogli come dizionario di DataFrame\n",
    "xls = pd.ExcelFile(\"../data/Superstore_mod_v1.xlsx\")\n",
    "df_dict = pd.read_excel(xls, sheet_name=None)  # None legge tutti i fogli\n",
    "\n",
    "# df_dict è un dizionario con chiavi = nomi dei fogli, valori = DataFrame\n",
    "for sheet_name, df in df_dict.items():\n",
    "    print(f\"Foglio: {sheet_name}\")  \n",
    "    globals()[f'df_{sheet_name.lower()}'] = df  # globals() in Python serve a restituire un dizionario contenente tutte le variabili globali \n",
    "                                                #attualmente disponibili nel contesto in cui viene chiamato\n",
    "\n",
    "display(df_countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f92f25d",
   "metadata": {},
   "source": [
    "FOGLIO COUNTRIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA STATE'''\n",
    "df_countries_raw=df_countries.drop_duplicates() #tolgo i duplicati da tutto il df_countries\n",
    "\n",
    "#FUNZIONE FUZZ SOGLIA 90\n",
    "df_countries_fuzz_90, dict_countries_fuzz_90= pulisci_stringhe_fuzz(df_countries_raw,'State',90) \n",
    "df_countries_fuzz_90 = df_countries_fuzz_90.rename(columns={'State_mod': 'State_fuzz_90'})\n",
    "df_countries_fuzz_90['State_fuzz_90'] = df_countries_fuzz_90['State_fuzz_90'].replace({\n",
    "    'NY': 'NEW YORK', \n",
    "    'NC': 'NORTH CAROLINA'\n",
    "})\n",
    "df_stati_corretti_fuzz_90=pd.DataFrame([(k, v) for k, v in dict_countries_fuzz_90.items() if len(v)>1], columns=['State', 'Variants_fuzz_90'])\n",
    "\n",
    "#FUNZIONE FUZZ SOGLIA 80\n",
    "df_countries_fuzz_80,dict_countries_fuzz_80= pulisci_stringhe_fuzz(df_countries_fuzz_90,'State',80)\n",
    "df_countries_fuzz_80= df_countries_fuzz_80.rename(columns={'State_mod':'State_fuzz_80'})\n",
    "df_countries_fuzz_80['State_fuzz_80']=df_countries_fuzz_80['State_fuzz_80'].replace({'NY': 'NEW YORK', 'NC': 'NORTH CAROLINA'}) \n",
    "df_stati_corretti_fuzz_80=pd.DataFrame([(k, v) for k, v in dict_countries_fuzz_80.items() if len(v)>1], columns=['State', 'Variants_fuzz_80'])\n",
    "\n",
    "\n",
    "#FUNZIONE LEV SOGLIA 1\n",
    "df_countries_lev_1, dict_countries_lev_1=pulisci_stringhe_lev(df_countries_fuzz_80,'State',1) #applico la funzione lev\n",
    "df_countries_lev_1= df_countries_lev_1.rename(columns={'State_mod':'State_lev_1'})\n",
    "df_countries_lev_1['State_lev_1']=df_countries_lev_1['State_lev_1'].replace({'NY': 'NEW YORK', 'NC': 'NORTH CAROLINA'}) \n",
    "#df_countries_lev_1, guardando gli output, è quello che si avvicina di più ad essere corretto\n",
    "state_map = {\n",
    "    'NC': 'NORTH CAROLINA'\n",
    "}\n",
    "df_countries_lev_1['State_lev_1'] = df_countries_lev_1['State'].map(state_map).fillna(df_countries_lev_1['State_lev_1'])\n",
    "df_stati_corretti_lev_1=pd.DataFrame([(k, v) for k, v in dict_countries_lev_1.items() if len(v) >1], columns=['State', 'Variants_lev_1'])\n",
    "\n",
    "#MERGE TRA I 3 DF\n",
    "df_merge_state_lev_fuzz90 = df_stati_corretti_fuzz_90.merge(df_stati_corretti_lev_1, on = 'State', how = 'outer')\n",
    "df_merge_state_lev_fuzz90_fuzz80= df_merge_state_lev_fuzz90.merge(df_stati_corretti_fuzz_80, on='State', how='outer' )\n",
    "\n",
    "'''LI CARICO SU EXCEL\n",
    "df_merge_state_lev_fuzz90_fuzz80.to_excel('/Users/alessandropavia/Desktop/Stage Reply/Sales Analysis 2/data/Countries_up.xlsx', sheet_name='Country', index=False)\n",
    "with pd.ExcelWriter('/Users/alessandropavia/Desktop/Stage Reply/Sales Analysis 2/data/fogli excel up/Countries_up.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "    df_merge_state_lev_fuzz90_fuzz80.to_excel(writer, sheet_name='Country(no len)', index=False) '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e2eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA CITY'''\n",
    "\n",
    "#FUNZIONE FUZZ CON SOGLIA 90\n",
    "df_countries_fuzz_city_90, dict_countries_fuzz_city_90= pulisci_stringhe_fuzz(df_countries_lev_1,'City',90) \n",
    "df_countries_fuzz_city_90 = df_countries_fuzz_city_90.rename(columns={'City_mod': 'City_fuzz_90'})\n",
    "df_countries_city_corrette_fuzz_90=pd.DataFrame([(k, v) for k, v in dict_countries_fuzz_city_90.items() ], columns=['City', 'Variants_fuzz_90'])\n",
    "\n",
    "\n",
    "#FUNZIONE FUZZ CON SOGLIA 95\n",
    "df_countries_fuzz_city_95,dict_countries_fuzz_city_95= pulisci_stringhe_fuzz(df_countries_fuzz_city_90,'City',95) \n",
    "df_countries_fuzz_city_95 = df_countries_fuzz_city_95.rename(columns={'City_mod': 'City_fuzz_95'})\n",
    "df_countries_fuzz_city_95['City_fuzz_95']= df_countries_fuzz_city_95['City_fuzz_95'].replace({'BOWLINGGREEN': 'BOWLING GREEN'}).replace({'WOODSTOK': 'WOODSTOCK'})\n",
    "df_countries_city_corrette_fuzz_95=pd.DataFrame([(k, v) for k, v in dict_countries_fuzz_city_95.items() ], columns=['City', 'Variants_fuzz_95'])\n",
    "df_countries_fuzz_city_95 = df_countries_fuzz_city_95.rename(columns={'City_fuzz_95': 'City_def'})\n",
    "df_countries_fuzz_city_95['City_def']=df_countries_fuzz_city_95['City_def'].replace({\n",
    "    'ENDERSON':'HENDERSON'\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "#FUNZIONE LEV CON SOGLIA 1\n",
    "df_countries_lev_city_1, dict_countries_lev_city_1= pulisci_stringhe_lev(df_countries_fuzz_city_95,'City',1) \n",
    "df_countries_lev_city_1 = df_countries_lev_city_1.rename(columns={'City_mod': 'City_lev_1'})\n",
    "df_countries_city_corrette_lev_1=pd.DataFrame([(k, v) for k, v in dict_countries_lev_city_1.items()], columns=['City', 'Variants_lev_1'])\n",
    "\n",
    "\n",
    "#MERGE TRA I 3 DF\n",
    "df_merge_city_fuzz_95_fuzz_90=df_countries_city_corrette_fuzz_95.merge(df_countries_city_corrette_fuzz_90, on='City', how='outer')\n",
    "df_merge_city_fuzz_95_fuzz_90_lev_1=df_countries_city_corrette_lev_1.merge(df_merge_city_fuzz_95_fuzz_90, on='City', how='outer')\n",
    "\n",
    "\n",
    "'''LI CARICO SU EXCEL\n",
    "with pd.ExcelWriter('/Users/alessandropavia/Desktop/Stage Reply/Sales Analysis 2/data/fogli excel up/Countries_up.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "    df_merge_city_fuzz_95_fuzz_90_lev_1.to_excel(writer, sheet_name='City(no len)', index=False)'''\n",
    "\n",
    "#METODO PIù GIUSTO: FUZZ CON SOGLIA=95 ci sono pochi errori in questa colonna city, ce ne sono molti nella colonna city del foglio orders, ancora da finire\n",
    "\n",
    "\n",
    "df_countries_lev_city_1=df_countries_lev_city_1[['Country','City','City_fuzz_90','City_def','City_lev_1','State','State_fuzz_90','State_fuzz_80','State_lev_1','Postal Code','Region']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a169070",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' ANCORA COLONNA CITY MA CON CONTROLLO DELLO STATO PER MODIFICARE LA CITTA: MODIFICO LA CITTA' SOLO SE HANNO LO STATO UGUALE'''\n",
    "\n",
    "#METODO FUZZ (2) CON SOGLIA 90\n",
    "df_countries_fuzz_city_90_c, dict_countries_fuzz_city_90_c= pulisci_stringhe_fuzz2(df_countries_lev_1,'City',90,'State_lev_1') \n",
    "df_countries_fuzz_city_90_c = df_countries_fuzz_city_90_c.rename(columns={'City_mod': 'City_fuzz_90'})\n",
    "df_countries_city_corrette_fuzz_90_c=pd.DataFrame([(k, v) for k, v in dict_countries_fuzz_city_90_c.items() ], columns=['City', 'Variants_fuzz_90'])\n",
    "\n",
    "\n",
    "#METODO FUZZ (2) CON SOGLIA 95\n",
    "df_countries_fuzz_city_95_c,dict_countries_fuzz_city_95_c= pulisci_stringhe_fuzz2(df_countries_fuzz_city_90_c,'City',95,'State_lev_1') \n",
    "df_countries_fuzz_city_95_c = df_countries_fuzz_city_95_c.rename(columns={'City_mod': 'City_fuzz_95'})\n",
    "df_countries_city_corrette_fuzz_95_c=pd.DataFrame([(k, v) for k, v in dict_countries_fuzz_city_95_c.items() ], columns=['City', 'Variants_fuzz_95'])\n",
    "\n",
    "\n",
    "\n",
    "#METODO LEV (2) CON SOGLIA 1\n",
    "df_countries_lev_city_1_c, dict_countries_lev_city_1_c= pulisci_stringhe_lev2(df_countries_fuzz_city_95_c,'City',1,'State_lev_1') \n",
    "df_countries_lev_city_1_c = df_countries_lev_city_1_c.rename(columns={'City_mod': 'City_lev_1'})\n",
    "df_countries_city_corrette_lev_1_c=pd.DataFrame([(k, v) for k, v in dict_countries_lev_city_1_c.items()], columns=['City', 'Variants_lev_1'])\n",
    "\n",
    "\n",
    "#MERGE TRA I 3 DF\n",
    "df_merge_city_fuzz_95_fuzz_90_c=df_countries_city_corrette_fuzz_95_c.merge(df_countries_city_corrette_fuzz_90_c, on='City', how='outer')\n",
    "df_merge_city_fuzz_95_fuzz_90_lev_1_c=df_countries_city_corrette_lev_1_c.merge(df_merge_city_fuzz_95_fuzz_90_c, on='City', how='outer')\n",
    "\n",
    "\n",
    "'''LI CARICO SU EXCEL\n",
    "with pd.ExcelWriter('/Users/alessandropavia/Desktop/Stage Reply/Sales Analysis 2/data/fogli excel up/Countries_up.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "   df_merge_city_fuzz_95_fuzz_90_lev_1_c.to_excel(writer, sheet_name='City_c', index=False)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31be0b1e",
   "metadata": {},
   "source": [
    "FOGLIO ORDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c376eca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_raw=df_orders\n",
    "df_orders_mod=df_orders_raw.copy().drop_duplicates()\n",
    "\n",
    "'''COLONNA SHIP MODE'''  \n",
    "#METODO LEV CON SOGLIA 5\n",
    "df_orders_ship_mode_lev_1, dict_orders_ship_mode_lev_1 =pulisci_stringhe_lev(df_orders_mod,'Ship Mode',5)\n",
    "\n",
    "\n",
    "'''LO CARICO SU EXCEL\n",
    "df_orders_ship_mode_corrette=pd.DataFrame([(k, v) for k, v in dict_orders_ship_mode_lev_1.items() if len(v)>1], columns=['Ship Mode', 'Variants_lev_1'])\n",
    "df_orders_ship_mode_corrette.to_excel('/Users/alessandropavia/Desktop/Stage Reply/Sales Analysis 2/data/Orders_up.xlsx', sheet_name='Ship Mode', index=False)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1dfc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA SALES'''\n",
    "\n",
    "df_orders_ship_mode_lev_1['Sales_mod']= df_orders_ship_mode_lev_1['Sales'].apply(pulisci_trattino) # sostitiusco il - con il . quando è in mezzo alla stringa\n",
    "df_orders_ship_mode_lev_1['Sales_mod'] = df_orders_ship_mode_lev_1['Sales_mod'].astype(str).str.strip() # rimuovo eventuali spazi bianchi e converto in string\n",
    "df_orders_ship_mode_lev_1['Sales_mod'] = df_orders_ship_mode_lev_1['Sales_mod'].str.replace(\",\", '.', regex=False).str.replace(\"'\", '.', regex=False).str.replace('a', '', regex=False) # sostituisco le virgole con punti (per uniformare la notazione decimale), il ' e la a\n",
    "\n",
    "df_orders_ship_mode_lev_1['Sales_mod'] = pd.to_numeric(df_orders_ship_mode_lev_1['Sales_mod'], errors='coerce')\n",
    "dict_sales_mod = creo_dict2(df_orders_ship_mode_lev_1,'Sales','Sales_mod')\n",
    "df_sales_mod_corrette=pd.DataFrame([(k, v) for k, v in dict_sales_mod.items()], columns=['Sales', 'Variants'])\n",
    "\n",
    "'''LO CARICO SU EXCEL\n",
    "with pd.ExcelWriter('/Users/alessandropavia/Desktop/Stage Reply/Sales Analysis 2/data/fogli excel up/Orders_up.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "    df_sales_mod_corrette.to_excel(writer, sheet_name='Sales.round2)', index=False)'''\n",
    "\n",
    "\n",
    "df_orders_ship_mode_lev_1=df_orders_ship_mode_lev_1[['Order ID','Order Date','Ship Date','Ship Mode','Ship Mode_mod','Customer ID','City','Postal Code','Product ID', 'Sales','Sales_mod','Quantity','Discount','Profit']] #avvicino Sales_mod a Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c51e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA QUANTITY'''\n",
    "\n",
    "df_orders_ship_mode_lev_1['Quantity_mod']=df_orders_ship_mode_lev_1['Quantity'].astype(str).apply(pulisci_trattino)\n",
    "df_orders_ship_mode_lev_1['Quantity_mod'] = df_orders_ship_mode_lev_1['Quantity_mod'].str.replace(\",\", '.', regex=False).str.replace(\"'\", '.', regex=False).str.replace('ì', '', regex=False) # sostituisco le virgole con punti (per uniformare la notazione decimale), il ' e il ì\n",
    "\n",
    "df_orders_ship_mode_lev_1['Quantity_mod'] = pd.to_numeric(df_orders_ship_mode_lev_1['Quantity_mod'], errors='coerce')\n",
    "dict_quantity_mod=creo_dict2(df_orders_ship_mode_lev_1,'Quantity','Quantity_mod')\n",
    "df_quantity_mod_corrette=pd.DataFrame([(k, v) for k, v in dict_quantity_mod.items() if len(v)>1], columns=['Quantity', 'Variants'])\n",
    "\n",
    "'''LO CARICO SU EXCEL\n",
    "with pd.ExcelWriter('/Users/alessandropavia/Desktop/Stage Reply/Sales Analysis 2/data/fogli excel up/Orders_up.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "   df_quantity_mod_corrette.to_excel(writer, sheet_name='Quantity.round', index=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d214d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA DISCOUNT'''\n",
    "\n",
    "df_orders_ship_mode_lev_1['Discount_mod'] = df_orders_ship_mode_lev_1['Discount'].astype(str).str.replace(\"ù\", '', regex=False).str.replace(\"%\", '', regex=False)\n",
    "\n",
    "df_orders_ship_mode_lev_1['Discount_mod']=pd.to_numeric(df_orders_ship_mode_lev_1['Discount_mod'],errors='coerce')\n",
    "dict_discount_mod=creo_dict(df_orders_ship_mode_lev_1,'Discount','Discount_mod')\n",
    "df_discount_mod_corretto=pd.DataFrame([(k, v) for k, v in dict_discount_mod.items() if len(v)>1], columns=['Discount', 'Variants'])\n",
    "\n",
    "'''LO CARICO SU EXCEL\n",
    "with pd.ExcelWriter('/Users/alessandropavia/Desktop/Stage Reply/Sales Analysis 2/data/Orders_up.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "   df_discount_mod_corretto.to_excel(writer, sheet_name='Discount', index=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea7fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA PROFIT'''\n",
    "\n",
    "df_orders_ship_mode_lev_1['Profit_mod']= df_orders_ship_mode_lev_1['Profit'].astype(str).apply(pulisci_trattino)\n",
    "df_orders_ship_mode_lev_1['Profit_mod'] = df_orders_ship_mode_lev_1['Profit_mod'].str.strip() # 1. Rimuovi eventuali spazi bianchi e converti in string\n",
    "df_orders_ship_mode_lev_1['Profit_mod'] = df_orders_ship_mode_lev_1['Profit_mod'].str.replace(\",\", '.', regex=False).str.replace(\"'\", '.', regex=False).str.replace('a', '', regex=False) # 2. Sostituisci le virgole con punti (per uniformare la notazione decimale)\n",
    "\n",
    "df_orders_ship_mode_lev_1['Profit_mod'] = pd.to_numeric(df_orders_ship_mode_lev_1['Profit_mod'], errors='coerce')\n",
    "dict_profit_mod=creo_dict2(df_orders_ship_mode_lev_1,'Profit','Profit_mod')\n",
    "df_profit_mod_corretto=pd.DataFrame([(k, v) for k, v in dict_profit_mod.items() if len(v)>1 ], columns=['Profit', 'Variants'])\n",
    "\n",
    "'''LO CARICO SU EXCEL\n",
    "with pd.ExcelWriter('/Users/alessandropavia/Desktop/Stage Reply/Sales Analysis 2/data/fogli excel up/Orders_up.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "  df_profit_mod_corretto.to_excel(writer, sheet_name='Profit.round2', index=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75676be",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA CITY'''\n",
    "\n",
    "#FUNZIONE FUZZ CON SOGLIA 90\n",
    "df_orders_fuzz_city_90, dict_orders_city_fuzz_90= pulisci_stringhe_fuzz(df_orders_ship_mode_lev_1,'City',90) \n",
    "df_orders_fuzz_city_90 = df_orders_fuzz_city_90.rename(columns={'City_mod': 'City_fuzz_90'})\n",
    "df_orders_fuzz_city_90['City_fuzz_90']= df_orders_fuzz_city_90['City_fuzz_90'].replace({'BOWLINGGREEN': 'BOWLING GREEN'})\n",
    "df_orders_city_corrette_fuzz_90=pd.DataFrame([(k, v) for k, v in dict_orders_city_fuzz_90.items() if len(v)>1], columns=['City', 'Variants_fuzz_90'])\n",
    "\n",
    "#FUNZIONE FUZZ CON SOGLIA 85\n",
    "df_orders_fuzz_city_85,dict_orders_city_fuzz_85=pulisci_stringhe_fuzz(df_orders_fuzz_city_90,'City',85) \n",
    "df_orders_fuzz_city_85 = df_orders_fuzz_city_85.rename(columns={'City_mod': 'City_fuzz_85'})\n",
    "df_orders_city_corrette_fuzz_85=pd.DataFrame([(k, v) for k, v in dict_orders_city_fuzz_85.items() if len(v)>1], columns=['City', 'Variants_fuzz_85'])\n",
    "\n",
    "\n",
    "\n",
    "#FUNZIONE LEV CON SOGLIA 1\n",
    "df_orders_lev_city_1,dict_orders_city_lev_1=pulisci_stringhe_lev(df_orders_fuzz_city_85,'City',1) # quello con meno errori\n",
    "df_orders_lev_city_1 = df_orders_lev_city_1.rename(columns={'City_mod': 'City_lev_1'})\n",
    "citta_da_ripristinare = ['Conroe', 'Redding', 'Camarillo', 'Englewood', 'Mason', 'Medford', 'Normal', 'Layton', 'Renton','Clifton','Edmond'] #SISTEMO LE CITTà CHE HA RIMOSSO MA NON DOVEVA\n",
    "mask = df_orders_lev_city_1['City'].isin(citta_da_ripristinare)\n",
    "df_orders_lev_city_1.loc[mask, 'City_lev_1'] = df_orders_lev_city_1.loc[mask, 'City'].str.upper()\n",
    "df_orders_city_corrette_lev_1=pd.DataFrame([(k, v) for k, v in dict_orders_city_lev_1.items() if len(v)>1], columns=['City', 'Variants_lev_1'])\n",
    "df_orders_lev_city_1['City_lev_1']=df_orders_lev_city_1['City_lev_1'].replace({\n",
    "    'ALECSANDRIA':'ALEXANDRIA',\n",
    "    'LA':'LOS ANGELES',\n",
    "    'MARLBOROUG':'MARLBOROUGH',\n",
    "    'FILADELPHIA':'PHILADELPHIA',\n",
    "    'NY CITY':'NEW YORK CITY',\n",
    "    'NYC':'NEW YORK CITY',\n",
    "    'EDINBURG':'EDINBURGH'\n",
    "})\n",
    "df_orders_lev_city_1=df_orders_lev_city_1.rename(columns={'City_lev_1':'City_def'})\n",
    "\n",
    "\n",
    "#FUNZIONE LEV CON SOGLIA 2\n",
    "df_orders_lev_city_2,dict_orders_city_lev_2=pulisci_stringhe_lev(df_orders_lev_city_1,'City',2) #non va bene, rimuove troppe città corrette\n",
    "df_orders_lev_city_2 = df_orders_lev_city_2.rename(columns={'City_mod': 'City_lev_2'})\n",
    "df_orders_city_corrette_lev_2=pd.DataFrame([(k, v) for k, v in dict_orders_city_lev_2.items() if len(v)>1], columns=['City', 'Variants_lev_2'])\n",
    "\n",
    "\n",
    "\n",
    "#FACCIO IL MERGE TRA I 4 DF\n",
    "df_merge_orders_city_fuzz_90_fuzz_85=df_orders_city_corrette_fuzz_90.merge(df_orders_city_corrette_fuzz_85, on='City',how='outer')\n",
    "df_merge_orders_city_fuzz_90_fuzz_85_lev_1=df_orders_city_corrette_lev_1.merge(df_merge_orders_city_fuzz_90_fuzz_85, on='City', how='outer')\n",
    "df_merge_orders_city_fuzz_90_fuzz_85_lev_1_lev_2=df_orders_city_corrette_lev_2.merge(df_merge_orders_city_fuzz_90_fuzz_85_lev_1, on='City', how='outer')\n",
    "\n",
    "\n",
    "'''LI CARICO SU EXCEL\n",
    "with pd.ExcelWriter('/Users/alessandropavia/Desktop/Stage Reply/Sales Analysis 2/data/Orders_up.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "   df_merge_orders_city_fuzz_90_fuzz_85_lev_1_lev_2.to_excel(writer, sheet_name='City', index=False)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66990a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA ORDER DATE'''\n",
    "df_date_pulite=uniforma_date_order_date(df_orders_lev_city_1,'Order Date','Ship Date')\n",
    "dict_order_date=creo_dict(df_date_pulite,'Order Date','Order Date_def')\n",
    "df_order_date_corrette=pd.DataFrame([(k, v) for k, v in dict_order_date.items()], columns=['Order Date', 'Variants'])\n",
    "\n",
    "#C'è ANCORA IL PROBLEMA DELLE RIGHE COME RIGA (422 OPPURE 513) CON LA DATA 13/13/2014 CHE ANCHE SE GUARDO LE DIVERSE COMBINAZIONI, NON HA NESSUNA DATA VALIDA, PER QUESTE DUE USO IL REPLACE\n",
    "df_date_pulite.loc[df_date_pulite['Order Date'] == '13/13/2014', ['Order Date_dt', 'Order Date_def']] = pd.Timestamp('2014-10-13')\n",
    "df_date_pulite.loc[df_date_pulite['Order Date'] == '25/13/2011', ['Order Date_dt', 'Order Date_def']] = pd.Timestamp('2011-11-25')\n",
    "df_date_pulite.loc[df_date_pulite['Order Date'] == '14/03/013', ['Order Date_dt', 'Order Date_def']] = pd.Timestamp('2013-03-14')\n",
    "\n",
    "df_date_pulite=df_date_pulite.drop(columns=['Order Date_dt','Ship Date_dt'])\n",
    "\n",
    "\n",
    "'''LO CARICO SU EXCEL\n",
    "with pd.ExcelWriter('/Users/alessandropavia/Desktop/Stage Reply/Sales Analysis 2/data/fogli excel up/Orders_up.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "   df_order_date_corrette.to_excel(writer, sheet_name='Order Date(no len)', index=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e64a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA SHIP DATE'''\n",
    "df_date_pulite2=uniforma_date_ship_date(df_date_pulite,'Ship Date', 'Order Date')\n",
    "dict_ship_date=creo_dict(df_date_pulite2,'Ship Date','Ship Date_def')\n",
    "df_ship_date_corrette=pd.DataFrame([(k, v) for k, v in dict_ship_date.items() ], columns=['Ship Date', 'Variants'])\n",
    "\n",
    "\n",
    "#devo pulire ancora le date in cui nessuna combniazione mi da una data possibile oppure altri errori\n",
    "df_date_pulite2.loc[df_date_pulite2['Ship Date'] == '02 Sép 2013', ['Ship Date_dt', 'Ship Date_def']] = pd.Timestamp('2013-09-02')\n",
    "df_date_pulite2.loc[df_date_pulite2['Ship Date'] == '22/0/2013', ['Ship Date_dt', 'Ship Date_def']] = pd.Timestamp('2013-10-22')\n",
    "df_date_pulite2.loc[df_date_pulite2['Ship Date'] == '28/13/2013', ['Ship Date_dt', 'Ship Date_def']] = pd.Timestamp('2013-05-28')\n",
    "df_date_pulite2.loc[df_date_pulite2['Ship Date'] == '15/09/013', ['Ship Date_dt', 'Ship Date_def']] = pd.Timestamp('2013-09-15')\n",
    "df_date_pulite2.loc[df_date_pulite2['Ship Date'] == '10_12_2012', ['Ship Date_dt', 'Ship Date_def']] = pd.Timestamp('2012-12-10')\n",
    "df_date_pulite2=df_date_pulite2.drop(columns=['Ship Date_dt','Order Date_dt'])\n",
    "\n",
    "\n",
    "'''LO CARICO SU EXCEL\n",
    "with pd.ExcelWriter('/Users/alessandropavia/Desktop/Stage Reply/Sales Analysis 2/data/fogli excel up/Orders_up.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "   df_ship_date_corrette.to_excel(writer, sheet_name='Ship Date(no len)', index=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c1bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA PRODUCT ID'''\n",
    "df_date_pulite2['Product ID_def']=df_date_pulite2['Product ID'].copy()\n",
    "\n",
    "df_date_pulite2['Product ID_def'] = (\n",
    "    df_date_pulite2['Product ID_def']\n",
    "    .str.replace('ù', '', regex=False)\n",
    "    .str.upper()\n",
    "    .str.replace(' ', '', regex=False)\n",
    "    .str.replace('-', '', regex=False)\n",
    ")\n",
    "\n",
    "\n",
    "#Applico il formato AAA-BB-1234567\n",
    "df_date_pulite2['Product ID_def'] = (\n",
    "    df_date_pulite2['Product ID_def'].str[:3] + '-' +\n",
    "    df_date_pulite2['Product ID_def'].str[3:5] + '-' +\n",
    "    df_date_pulite2['Product ID_def'].str[5:]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04aaea1",
   "metadata": {},
   "source": [
    "FOGLIO PRODUCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14012e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_raw = df_products\n",
    "df_products_mod=df_products_raw.copy().drop_duplicates()\n",
    "\n",
    "\n",
    "'''COLONNA CATEGORY'''\n",
    "#METODO LEV CON SOGLIA 6\n",
    "df_products_cat_lev_6, dict_products_cat=pulisci_stringhe_lev(df_products_mod,'Category',6)\n",
    "df_products_cat_corrette=pd.DataFrame([(k, v) for k, v in dict_products_cat.items()  if len(v)>1], columns=['Category', 'Variants'])\n",
    "\n",
    "'''LO CARICO SU EXCEL\n",
    "df_products_cat_corrette.to_excel('/Users/alessandropavia/Desktop/Stage Reply/Sales Analysis 2/data/Products_up.xlsx', sheet_name='Category', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49eff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA SUB-CATEGORY'''\n",
    "#METODO LEV CON SOGLIA 2\n",
    "df_products_sub_lev2, dict_products_sub=pulisci_stringhe_lev(df_products_cat_lev_6,'Sub-Category',2)\n",
    "df_products_sub_corrette=pd.DataFrame([(k, v) for k, v in dict_products_sub.items() if len(v)>1], columns=['Sub-Category', 'Variants'])\n",
    "\n",
    "'''LO CARICO SU EXCEL\n",
    "#with pd.ExcelWriter('/Users/alessandropavia/Desktop/Stage Reply/Sales Analysis 2/data/Products_up.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "#   df_products_sub_corrette.to_excel(writer, sheet_name='Sub-Category', index=False)'''\n",
    "\n",
    "df_products_sub_lev2=df_products_sub_lev2[['Product ID','Category','Category_mod','Sub-Category','Sub-Category_mod','Product Name']]#cambio ordine delle colonnne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d519730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA PRODUCT-ID'''\n",
    "df_products_sub_lev2['Product ID_mod']=df_products_sub_lev2['Product ID']\n",
    "\n",
    "for index, row in df_products_sub_lev2.iterrows():\n",
    "    product_id = row['Product ID_mod']\n",
    "    category = row['Category_mod']\n",
    "    sub_category= row['Sub-Category_mod']\n",
    "\n",
    "    if pd.notnull(product_id) and '-' in product_id:\n",
    "        product_cat = product_id.split('-')[0]\n",
    "        product_sub_cat = product_id.split('-')[1]\n",
    "\n",
    "        if product_cat != category[:3]:  # primi 3 caratteri della stringa\n",
    "            print(f'La riga {index} è sbagliata: ID = {product_id}, Categoria = {category}')\n",
    "        \n",
    "        elif product_sub_cat != sub_category[:2]:\n",
    "            print(f'La riga {index} è sbagliata: ID= {product_id}, Sub-Category= {product_sub_cat}')\n",
    "            #OK NO ERRORI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e8a82c",
   "metadata": {},
   "source": [
    "FOGLIO CUSTOMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e834bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers_raw=df_customers\n",
    "df_customers_mod=df_customers_raw.copy().drop_duplicates()\n",
    "\n",
    "'''COLONNA SEGMENT'''\n",
    "#METODO LEV CON SOGLIA 2\n",
    "df_customers_lev_2, dict_segment=pulisci_stringhe_lev(df_customers_mod,'Segment',2)\n",
    "df_customers_segment_corretto=pd.DataFrame([(k, v) for k, v in dict_segment.items() if len(v)>1], columns=['Segment', 'Variants'])\n",
    "\n",
    "'''LO CARICO SU EXCEL\n",
    "df_customers_segment_corretto.to_excel('/Users/alessandropavia/Desktop/Stage Reply/Sales Analysis 2/data/Customers_up.xlsx', sheet_name='Segment', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f93802",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers_lev_2['Customer ID_mod']=df_customers_lev_2['Customer ID'].copy()\n",
    "df_customers_lev_2['Customer Name_mod'] = (df_customers_lev_2['Customer Name'].str.replace('-', ' ', regex=False).str.replace('_', ' ', regex=False))\n",
    "\n",
    "'''COLONNA CUSTOMER ID'''\n",
    "for index in df_customers_lev_2.index:\n",
    "    customer_id = df_customers_lev_2.at[index, 'Customer ID_mod']\n",
    "    customer_full_name = df_customers_lev_2.at[index, 'Customer Name_mod']\n",
    "    \n",
    "    if pd.notnull(customer_id) and '-' in customer_id:\n",
    "        customer_id_first_letter_name = customer_id[0]\n",
    "        customer_id_first_letter_surname = customer_id[1]\n",
    "\n",
    "        parts = customer_full_name.strip().split()\n",
    "        if len(parts) >= 2:\n",
    "            first_letter_name = parts[0][0]\n",
    "            first_letter_surname = parts[1][0]\n",
    "        else:\n",
    "            # Gestione nome-cognome attaccati tipo 'CoreyLock'\n",
    "            split_parts = re.split(r'(?=[A-Z])', customer_full_name)\n",
    "            if len(split_parts) >= 3:\n",
    "                first_letter_name = split_parts[1][0]\n",
    "                first_letter_surname = split_parts[2][0]\n",
    "            else:\n",
    "                continue  # nome non gestibile, salta\n",
    "\n",
    "        if (\n",
    "            customer_id_first_letter_name != first_letter_name or\n",
    "            customer_id_first_letter_surname != first_letter_surname\n",
    "        ):\n",
    "            print(f'La riga {index} è errata: il Customer ID è {customer_id}, il Customer Name è {customer_full_name}')\n",
    "            df_customers_lev_2.at[index, 'Customer ID_mod'] = first_letter_name + first_letter_surname + customer_id[2:]\n",
    "\n",
    "            \n",
    "dict_customers_id_corretti=creo_dict(df_customers_lev_2,'Customer ID','Customer ID_mod')\n",
    "df_customers_id_corretti=pd.DataFrame([(k, v) for k, v in dict_customers_id_corretti.items()], columns=['Customer ID', 'Variants'])\n",
    "\n",
    "'''LO CARICO SU EXCEL\n",
    "with pd.ExcelWriter('/Users/alessandropavia/Desktop/Stage Reply/Sales Analysis 2/data/Customers_up.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "   df_customers_id_corretti.to_excel(writer, sheet_name='Customer ID', index=False)'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a32b7ae",
   "metadata": {},
   "source": [
    "MERGE TRA ORDERS E COUNTRIES SULLE CITTA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943fc410",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_pulite2['Postal Code_mod'] = df_date_pulite2['Postal Code'].astype(str)\n",
    "df_countries_fuzz_city_95['Postal Code_mod'] = df_countries_fuzz_city_95['Postal Code'].astype(str)\n",
    "\n",
    "df_merge = pd.merge(df_date_pulite2.drop(columns=['Order Date','Ship Date','Ship Mode','Postal Code','Sales','Quantity','Discount','Profit','City_fuzz_90','City_fuzz_85','City','Product ID']), df_countries_fuzz_city_95.drop(columns=['City','Postal Code','State_fuzz_90','State_fuzz_80','State','City_fuzz_90']), how='left', on=(['City_def','Postal Code_mod']))\n",
    "pd.set_option('display.max_columns', None) #visualizzo tutte le colonne dei df\n",
    "\n",
    "\n",
    "df_merge=df_merge[['Order ID','Product ID_def','Customer ID','Country','Region','State_lev_1','City_def','Postal Code_mod','Order Date_def','Ship Date_def','Ship Mode_mod','Sales_mod','Profit_mod','Discount_mod','Quantity_mod']]\n",
    "df_merge=df_merge.rename(columns={'State_lev_1':'State_def',\n",
    "                                  'Customer ID':'Customer ID_def',\n",
    "                                  'Postal Code_mod':'Postal Code_def',\n",
    "                                  'Profit_mod':'Profit_def',\n",
    "                                  'Sales_mod':'Sales_def',\n",
    "                                  'Ship Mode_mod':'Ship Mode_def',\n",
    "                                  'Discount_mod':'Discount_def',\n",
    "                                  'Quantity_mod':'Quantity_def',\n",
    "                                  })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada47367",
   "metadata": {},
   "source": [
    "MERGE TRA IL DF CON GIA ORDERS E COUNTRIES E IL DF CUSTOMERS CON CHIAVE CUSTOMER ID_DEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f417ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers_lev_2=df_customers_lev_2.rename(columns={'Customer ID_mod':'Customer ID_def'})\n",
    "df_merge_customers=df_merge.merge(df_customers_lev_2.drop(columns=['Segment','Customer ID','Customer Name']), on='Customer ID_def', how='left')\n",
    "df_merge_customers=df_merge_customers.rename(columns={'Segment_mod':'Segment_def',\n",
    "                                                      'Customer Name_mod':'Customer Name_def'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20852bb7",
   "metadata": {},
   "source": [
    "MERGE TRA DF GIA CON I 3 FOGLI E IL DF PRODUCTS CON CHIAVE PRODUCT ID_DEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cee043",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_sub_lev2=df_products_sub_lev2.rename(columns={'Product ID_mod':'Product ID_def'})\n",
    "\n",
    "df_merge_def=df_merge_customers.merge(df_products_sub_lev2.drop(columns=['Product Name','Category','Sub-Category','Product ID']).drop_duplicates(), on='Product ID_def',how='left')\n",
    "\n",
    "\n",
    "'''PULISCO ERRORI SU PRODUCT ID'''\n",
    "df_merge_def=df_merge_def.rename(columns={'Product ID_def':'Product ID_mod'})\n",
    "df_merge_def['Product ID']=df_merge_def['Product ID_mod'].copy()\n",
    "\n",
    "\n",
    "for index, row in df_merge_def.iterrows():\n",
    "    if pd.isna(row['Category_mod']):\n",
    "        product_parts=row['Product ID_mod'].split('-')\n",
    "        categoria=product_parts[0]\n",
    "        sotto_categoria=product_parts[1]\n",
    "        codice_numerico=product_parts[2]\n",
    "        if categoria.startswith('OF') and categoria != 'OFF' and len(categoria) == 3:\n",
    "            nuova_categoria=categoria[0]+'F'+categoria[1]\n",
    "            ultima_lettera_categoria=categoria[2]\n",
    "            \n",
    "            nuova_sotto_categoria=ultima_lettera_categoria + sotto_categoria[:-1]\n",
    "            ultima_lettera_sotto_categoria= sotto_categoria[-1]\n",
    "\n",
    "            nuovo_codice_numerico=ultima_lettera_sotto_categoria + codice_numerico\n",
    "\n",
    "            nuovo_product_id = f\"{nuova_categoria}-{nuova_sotto_categoria}-{nuovo_codice_numerico}\"\n",
    "\n",
    "            # Applica la modifica\n",
    "            df_merge_def.at[index, 'Product ID_mod'] = nuovo_product_id\n",
    "\n",
    "#FACCIO IL REPLACE PER ERRORI CHE NON SO COME CORREGGERE\n",
    "df_merge_def['Product ID_mod']=df_merge_def['Product ID_mod'].replace('FUR-BO-10000468','FUR-BO-1000468').replace('FUR-U1-0001935','FUR-FU-10001935').replace('OFF-BI-1000948','OFF-BI-10000948').replace('OFF-ST-10001128','OFF-ST-10001228').replace('OFF-21-0002049','OFF-BI-10002049').replace('FUR-BO-10002206','FUR-BO-1002206').replace('TEC-PH-O10002555','TEC-PH-10002555').replace('OFF-PA-1001274','OFF-PA-10001274').replace('OFF-PA-1003724','OFF-PA-10003724')\n",
    "\n",
    "df_merge_def=df_merge_def.rename(columns={'Product ID_mod':'Product ID_def'})\n",
    "df_merge_def=df_merge_def.drop(columns=['Product ID','Category_mod','Sub-Category_mod']).merge(df_products_sub_lev2.drop(columns=['Product Name','Category','Sub-Category','Product ID']).drop_duplicates(), on='Product ID_def',how='left')\n",
    "df_merge_def=df_merge_def.rename(columns={'Category_mod':'Category_def', 'Sub-Category_mod':'Sub-Category_def'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a1e539",
   "metadata": {},
   "source": [
    "CREO CSV CON IL DF FINALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b124d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merge_def.to_csv('MasterTable.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
